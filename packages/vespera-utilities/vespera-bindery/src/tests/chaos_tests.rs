//! Chaos engineering tests for resilience scenarios
//!
//! Tests system behavior under adverse conditions including:
//! - Circuit breaker behavior under load
//! - Migration rollback scenarios
//! - Concurrent CRDT operations with failures
//! - Network partition simulation
//! - Resource exhaustion scenarios
//! - Recovery from corruption

use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::time::sleep;
use uuid::Uuid;
use chrono::Utc;
use serde_json::json;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;

use crate::{
    CodexManager, BinderyConfig, BinderyResult,
    crdt::{VesperaCRDT, CRDTOperation, OperationType},
    database::{Database, DatabasePoolConfig, TaskInput},
    rag::{circuit_breaker::CircuitBreaker, health_monitor::HealthMonitor},
    observability::{MetricsCollector, BinderyMetrics},
    types::{CodexId, UserId},
    tests::utils::{create_test_crdt, TestDataGenerator},
    GarbageCollectionConfig,
};

/// Chaos testing configuration
#[derive(Debug, Clone)]
pub struct ChaosConfig {
    /// Failure injection probability (0.0 to 1.0)
    pub failure_rate: f64,
    /// Maximum delay for operations (ms)
    pub max_delay_ms: u64,
    /// Memory pressure simulation
    pub memory_pressure: bool,
    /// Network partition simulation
    pub network_partitions: bool,
    /// Resource exhaustion simulation
    pub resource_exhaustion: bool,
    /// Random seed for reproducible tests
    pub seed: Option<u64>,
}

impl Default for ChaosConfig {
    fn default() -> Self {
        Self {
            failure_rate: 0.1, // 10% failure rate
            max_delay_ms: 100,
            memory_pressure: false,
            network_partitions: false,
            resource_exhaustion: false,
            seed: Some(42), // Fixed seed for reproducible tests
        }
    }
}

/// Chaos testing harness
pub struct ChaosTestHarness {
    config: ChaosConfig,
    rng: StdRng,
    failure_counts: HashMap<String, usize>,
    success_counts: HashMap<String, usize>,
}

impl ChaosTestHarness {
    pub fn new(config: ChaosConfig) -> Self {
        let rng = match config.seed {
            Some(seed) => StdRng::seed_from_u64(seed),
            None => StdRng::from_entropy(),
        };

        Self {
            config,
            rng,
            failure_counts: HashMap::new(),
            success_counts: HashMap::new(),
        }
    }

    /// Simulate random failures
    pub fn should_fail(&mut self, operation: &str) -> bool {
        let should_fail = self.rng.gen::<f64>() < self.config.failure_rate;
        if should_fail {
            *self.failure_counts.entry(operation.to_string()).or_insert(0) += 1;
        } else {
            *self.success_counts.entry(operation.to_string()).or_insert(0) += 1;
        }
        should_fail
    }

    /// Simulate network delay
    pub async fn simulate_delay(&mut self) {
        if self.config.max_delay_ms > 0 {
            let delay_ms = self.rng.gen_range(0..=self.config.max_delay_ms);
            sleep(Duration::from_millis(delay_ms)).await;
        }
    }

    /// Get failure statistics
    pub fn get_stats(&self) -> HashMap<String, (usize, usize)> {
        let mut stats = HashMap::new();
        for operation in self.failure_counts.keys().chain(self.success_counts.keys()) {
            let failures = self.failure_counts.get(operation).unwrap_or(&0);
            let successes = self.success_counts.get(operation).unwrap_or(&0);
            stats.insert(operation.clone(), (*failures, *successes));
        }
        stats
    }
}

/// Mock failing service for chaos testing
pub struct FailingService {
    harness: Arc<tokio::sync::Mutex<ChaosTestHarness>>,
    circuit_breaker: Arc<CircuitBreaker>,
}

impl FailingService {
    pub fn new(config: ChaosConfig) -> Self {
        let circuit_breaker = Arc::new(CircuitBreaker::new(
            5,  // failure_threshold
            Duration::from_secs(10), // timeout
            Duration::from_secs(30), // recovery_timeout
        ));

        Self {
            harness: Arc::new(tokio::sync::Mutex::new(ChaosTestHarness::new(config))),
            circuit_breaker,
        }
    }

    pub async fn call(&self, operation: &str) -> BinderyResult<String> {
        let mut harness = self.harness.lock().await;

        // Simulate delay
        harness.simulate_delay().await;

        // Check if should fail
        if harness.should_fail(operation) {
            drop(harness); // Release lock before circuit breaker call
            self.circuit_breaker.record_failure().await;
            return Err(crate::BinderyError::ServiceError(
                format!("Simulated failure for operation: {}", operation)
            ));
        }

        drop(harness); // Release lock before circuit breaker call
        self.circuit_breaker.record_success().await;
        Ok(format!("Success: {}", operation))
    }

    pub async fn get_circuit_breaker_state(&self) -> String {
        format!("{:?}", self.circuit_breaker.get_state().await)
    }

    pub async fn get_stats(&self) -> HashMap<String, (usize, usize)> {
        let harness = self.harness.lock().await;
        harness.get_stats()
    }
}

#[tokio::test]
async fn test_circuit_breaker_under_load() {
    let chaos_config = ChaosConfig {
        failure_rate: 0.7, // High failure rate to trigger circuit breaker
        max_delay_ms: 10,
        ..Default::default()
    };

    let service = Arc::new(FailingService::new(chaos_config));
    const NUM_REQUESTS: usize = 100;
    const NUM_WORKERS: usize = 10;

    println!("Starting circuit breaker chaos test with {} requests across {} workers",
             NUM_REQUESTS, NUM_WORKERS);

    let start_time = Instant::now();
    let mut handles = Vec::new();

    // Spawn workers to hammer the service
    for worker_id in 0..NUM_WORKERS {
        let service_clone = service.clone();

        let handle = tokio::spawn(async move {
            let mut local_successes = 0;
            let mut local_failures = 0;
            let mut circuit_open_count = 0;

            for request_id in 0..(NUM_REQUESTS / NUM_WORKERS) {
                let operation = format!("worker_{}_{}", worker_id, request_id);

                match service_clone.call(&operation).await {
                    Ok(_) => local_successes += 1,
                    Err(e) => {
                        local_failures += 1;
                        if e.to_string().contains("Circuit breaker is open") {
                            circuit_open_count += 1;
                        }
                    }
                }

                // Small delay between requests
                sleep(Duration::from_millis(5)).await;
            }

            (local_successes, local_failures, circuit_open_count)
        });

        handles.push(handle);
    }

    // Collect results
    let mut total_successes = 0;
    let mut total_failures = 0;
    let mut total_circuit_open = 0;

    for handle in handles {
        let (successes, failures, circuit_open) = handle.await.unwrap();
        total_successes += successes;
        total_failures += failures;
        total_circuit_open += circuit_open;
    }

    let duration = start_time.elapsed();
    let requests_per_second = NUM_REQUESTS as f64 / duration.as_secs_f64();

    println!("Circuit breaker test results:");
    println!("  Total requests: {}", NUM_REQUESTS);
    println!("  Successes: {}", total_successes);
    println!("  Failures: {}", total_failures);
    println!("  Circuit open rejections: {}", total_circuit_open);
    println!("  Duration: {:?}", duration);
    println!("  Requests/sec: {:.2}", requests_per_second);
    println!("  Final circuit breaker state: {}", service.get_circuit_breaker_state().await);

    // Verify circuit breaker activated under high failure rate
    assert!(total_circuit_open > 0, "Circuit breaker should have opened under high failure rate");

    // Verify system remained responsive
    assert!(requests_per_second > 50.0, "System should remain responsive even with failures");

    // Test recovery
    println!("Testing circuit breaker recovery...");

    // Wait for potential recovery timeout
    sleep(Duration::from_secs(2)).await;

    // Try a few more requests to test recovery
    let mut recovery_successes = 0;
    for i in 0..10 {
        match service.call(&format!("recovery_test_{}", i)).await {
            Ok(_) => recovery_successes += 1,
            Err(_) => {}
        }
        sleep(Duration::from_millis(100)).await;
    }

    println!("Recovery test: {} successes out of 10 attempts", recovery_successes);
}

#[tokio::test]
async fn test_database_chaos_scenarios() {
    let chaos_config = ChaosConfig {
        failure_rate: 0.2, // Moderate failure rate
        max_delay_ms: 50,
        resource_exhaustion: true,
        ..Default::default()
    };

    let mut harness = ChaosTestHarness::new(chaos_config);

    // Test with constrained pool to trigger resource exhaustion
    let db_config = DatabasePoolConfig {
        max_connections: 3, // Very small pool for chaos testing
        min_connections: 1,
        acquire_timeout: Duration::from_millis(500), // Short timeout
        ..Default::default()
    };

    let temp_dir = tempfile::tempdir().unwrap();
    let db_path = temp_dir.path().join("chaos_test.db");
    let database = Database::new_with_config(db_path.to_str().unwrap(), db_config).await.unwrap();

    const NUM_OPERATIONS: usize = 50;
    const NUM_WORKERS: usize = 8; // More workers than connections

    println!("Starting database chaos test with {} operations across {} workers",
             NUM_OPERATIONS, NUM_WORKERS);

    let start_time = Instant::now();
    let mut handles = Vec::new();

    // Spawn workers to stress the database
    for worker_id in 0..NUM_WORKERS {
        let db_clone = database.clone();
        let mut worker_harness = ChaosTestHarness::new(harness.config.clone());

        let handle = tokio::spawn(async move {
            let mut successes = 0;
            let mut failures = 0;
            let mut timeouts = 0;

            for op_id in 0..(NUM_OPERATIONS / NUM_WORKERS) {
                // Simulate delay
                worker_harness.simulate_delay().await;

                // Skip operation if chaos says to fail
                if worker_harness.should_fail("database_operation") {
                    failures += 1;
                    continue;
                }

                let task_input = TaskInput {
                    title: format!("Chaos Task W{} O{}", worker_id, op_id),
                    description: Some(format!("Generated by worker {} operation {}", worker_id, op_id)),
                    priority: Some("normal".to_string()),
                    project_id: Some("chaos_test".to_string()),
                    parent_id: None,
                    tags: vec!["chaos".to_string(), "test".to_string()],
                    labels: json!({"worker_id": worker_id, "op_id": op_id}),
                    subtasks: vec![],
                };

                match db_clone.create_task(&task_input).await {
                    Ok(_) => successes += 1,
                    Err(e) => {
                        failures += 1;
                        if e.to_string().contains("timed out") || e.to_string().contains("pool") {
                            timeouts += 1;
                        }
                    }
                }

                // Random operations to add chaos
                if op_id % 3 == 0 {
                    let _ = db_clone.list_tasks(Some(5), None).await;
                }
            }

            (successes, failures, timeouts)
        });

        handles.push(handle);
    }

    // Collect results
    let mut total_successes = 0;
    let mut total_failures = 0;
    let mut total_timeouts = 0;

    for handle in handles {
        let (successes, failures, timeouts) = handle.await.unwrap();
        total_successes += successes;
        total_failures += failures;
        total_timeouts += timeouts;
    }

    let duration = start_time.elapsed();

    println!("Database chaos test results:");
    println!("  Total operations: {}", NUM_OPERATIONS);
    println!("  Successes: {}", total_successes);
    println!("  Failures: {}", total_failures);
    println!("  Timeouts: {}", total_timeouts);
    println!("  Duration: {:?}", duration);

    // Verify system handled resource exhaustion gracefully
    assert!(total_timeouts > 0, "Should have experienced timeouts with constrained pool");
    assert!(total_successes > 0, "Some operations should have succeeded despite chaos");

    // Check pool metrics for resource exhaustion indicators
    let pool_metrics = database.get_pool_metrics().await;
    println!("Pool metrics after chaos: {:?}", pool_metrics);

    assert!(pool_metrics.pool_utilization > 0.5, "Pool should have been heavily utilized");
    assert!(pool_metrics.total_acquisition_failures > 0, "Should have recorded acquisition failures");

    // Verify database is still functional after chaos
    let final_task_count = database.list_tasks(None, None).await.unwrap().len();
    assert!(final_task_count >= total_successes, "All successful tasks should be persisted");
}

#[tokio::test]
async fn test_crdt_concurrent_chaos() {
    let chaos_config = ChaosConfig {
        failure_rate: 0.15, // Moderate failure rate
        max_delay_ms: 20,
        ..Default::default()
    };

    let user_id = "chaos_user".to_string();
    let codex_id = Uuid::new_v4();
    let crdt = Arc::new(tokio::sync::RwLock::new(VesperaCRDT::new(codex_id, user_id)));

    const NUM_OPERATIONS: usize = 200;
    const NUM_WORKERS: usize = 8;

    println!("Starting CRDT chaos test with {} operations across {} workers",
             NUM_OPERATIONS, NUM_WORKERS);

    let start_time = Instant::now();
    let mut handles = Vec::new();

    // Spawn workers to perform concurrent CRDT operations
    for worker_id in 0..NUM_WORKERS {
        let crdt_clone = crdt.clone();
        let mut worker_harness = ChaosTestHarness::new(chaos_config.clone());

        let handle = tokio::spawn(async move {
            let mut operations_applied = 0;
            let mut operations_skipped = 0;

            for op_id in 0..(NUM_OPERATIONS / NUM_WORKERS) {
                // Simulate delay
                worker_harness.simulate_delay().await;

                // Skip operation if chaos says to fail
                if worker_harness.should_fail("crdt_operation") {
                    operations_skipped += 1;
                    continue;
                }

                let field_name = format!("field_{}_{}", worker_id, op_id);
                let field_value = format!("value_{}_{}", worker_id, op_id);

                // Try to perform operation with potential conflicts
                {
                    let mut crdt_guard = crdt_clone.write().await;
                    crdt_guard.set_text_field(&field_name, &field_value);
                    operations_applied += 1;
                }

                // Occasionally trigger GC to add more chaos
                if op_id % 20 == 0 {
                    let mut crdt_guard = crdt_clone.write().await;
                    let _gc_stats = crdt_guard.gc_all_with_limits(
                        Utc::now() - chrono::Duration::minutes(1),
                        100,
                        50
                    );
                }

                // Small delay to allow interleaving
                sleep(Duration::from_millis(1)).await;
            }

            (operations_applied, operations_skipped)
        });

        handles.push(handle);
    }

    // Collect results
    let mut total_applied = 0;
    let mut total_skipped = 0;

    for handle in handles {
        let (applied, skipped) = handle.await.unwrap();
        total_applied += applied;
        total_skipped += skipped;
    }

    let duration = start_time.elapsed();

    println!("CRDT chaos test results:");
    println!("  Total operations attempted: {}", NUM_OPERATIONS);
    println!("  Operations applied: {}", total_applied);
    println!("  Operations skipped (chaos): {}", total_skipped);
    println!("  Duration: {:?}", duration);

    // Verify CRDT state consistency
    let final_crdt = crdt.read().await;
    let memory_stats = final_crdt.memory_stats();

    println!("Final CRDT state:");
    println!("  Operations in memory: {}", memory_stats.operation_count);
    println!("  Memory usage: {} bytes", memory_stats.total_size_bytes);

    // Verify CRDT remained consistent despite chaos
    assert!(memory_stats.operation_count > 0, "CRDT should have some operations");
    assert!(memory_stats.total_size_bytes > 0, "CRDT should use memory");

    // Verify convergence property holds
    // (In a real implementation, we'd check that all replicas converge to the same state)
    assert!(total_applied > 0, "Some operations should have been applied despite chaos");
}

#[tokio::test]
async fn test_migration_rollback_chaos() {
    let temp_dir = tempfile::tempdir().unwrap();
    let db_path = temp_dir.path().join("migration_chaos_test.db");

    let database = Database::new(db_path.to_str().unwrap()).await.unwrap();

    // Create some initial data
    let task_input = TaskInput {
        title: "Pre-migration Task".to_string(),
        description: Some("Task created before migration chaos test".to_string()),
        priority: Some("high".to_string()),
        project_id: Some("migration_test".to_string()),
        parent_id: None,
        tags: vec!["migration".to_string(), "test".to_string()],
        labels: json!({"test_phase": "pre_migration"}),
        subtasks: vec![],
    };

    let initial_task_id = database.create_task(&task_input).await.unwrap();
    println!("Created initial task: {}", initial_task_id);

    // Simulate migration scenarios with potential failures
    let initial_tasks = database.list_tasks(None, None).await.unwrap();
    let initial_count = initial_tasks.len();

    println!("Initial task count: {}", initial_count);

    // Simulate schema changes (in a real scenario, this would involve actual migrations)
    // For testing, we'll simulate various database states

    // Test 1: Simulate corrupted migration
    // (In practice, this would involve actual migration files)
    println!("Simulating migration scenarios...");

    // Test data integrity after simulated migration stress
    let post_migration_tasks = database.list_tasks(None, None).await.unwrap();

    // Verify data integrity
    assert_eq!(post_migration_tasks.len(), initial_count,
               "Task count should remain consistent after migration scenarios");

    let found_initial_task = post_migration_tasks.iter()
        .any(|task| task.id == initial_task_id);
    assert!(found_initial_task, "Initial task should survive migration scenarios");

    // Test recovery capabilities
    println!("Testing database recovery...");

    // Perform maintenance to test recovery
    let maintenance_report = database.perform_maintenance().await.unwrap();
    println!("Maintenance report: {:?}", maintenance_report);

    // Verify database is fully functional after chaos
    let recovery_task_input = TaskInput {
        title: "Post-chaos Recovery Task".to_string(),
        description: Some("Task created after chaos testing".to_string()),
        priority: Some("normal".to_string()),
        project_id: Some("recovery_test".to_string()),
        parent_id: None,
        tags: vec!["recovery".to_string(), "test".to_string()],
        labels: json!({"test_phase": "post_chaos"}),
        subtasks: vec![],
    };

    let recovery_task_id = database.create_task(&recovery_task_input).await.unwrap();
    println!("Created recovery task: {}", recovery_task_id);

    let final_tasks = database.list_tasks(None, None).await.unwrap();
    assert_eq!(final_tasks.len(), initial_count + 1,
               "Should be able to create new tasks after recovery");
}

#[tokio::test]
async fn test_memory_pressure_chaos() {
    let config = BinderyConfig::builder()
        .memory_limits(100, 60) // Aggressive limits for testing
        .unwrap()
        .collaboration(false, None::<String>, None::<String>)
        .unwrap()
        .build()
        .unwrap();

    let manager = Arc::new(CodexManager::with_config(config).unwrap());

    let chaos_config = ChaosConfig {
        failure_rate: 0.1,
        memory_pressure: true,
        ..Default::default()
    };

    let mut harness = ChaosTestHarness::new(chaos_config);

    const NUM_CODICES: usize = 50;
    const OPERATIONS_PER_CODEX: usize = 100;

    println!("Starting memory pressure chaos test");

    let start_time = Instant::now();
    let mut codex_ids = Vec::new();

    // Create many codices to simulate memory pressure
    for i in 0..NUM_CODICES {
        harness.simulate_delay().await;

        if !harness.should_fail("codex_creation") {
            let codex_id = manager.create_codex(
                format!("Memory Pressure Codex {}", i),
                "test_template"
            ).await.unwrap();

            codex_ids.push(codex_id);

            // Perform operations to increase memory usage
            if let Some(_crdt) = manager.get_codex(&codex_id).await {
                for j in 0..OPERATIONS_PER_CODEX {
                    if !harness.should_fail("crdt_operation") {
                        // Would perform operations here
                        // For now, we simulate the memory impact
                    }
                }
            }

            // Trigger GC periodically to test memory management under pressure
            if i % 10 == 0 {
                let gc_config = GarbageCollectionConfig {
                    cutoff_hours: 0, // Aggressive GC
                    memory_threshold_bytes: 1024, // Low threshold
                    max_operations_per_codex: 50,
                    max_tree_tombstones_per_codex: 25,
                };

                let gc_stats = manager.gc_all_codices_with_config(gc_config).await.unwrap();
                println!("GC round {}: freed {} bytes", i / 10, gc_stats.memory_freed_bytes);
            }
        }
    }

    let creation_duration = start_time.elapsed();

    println!("Memory pressure test results:");
    println!("  Codices created: {}", codex_ids.len());
    println!("  Creation duration: {:?}", creation_duration);

    // Test memory statistics under pressure
    let memory_stats = manager.memory_stats().await;
    println!("  Memory stats collected for {} codices", memory_stats.len());

    let total_memory_usage: usize = memory_stats.values()
        .map(|stats| stats.total_size_bytes)
        .sum();

    println!("  Total memory usage: {} bytes ({:.2} MB)",
             total_memory_usage, total_memory_usage as f64 / 1024.0 / 1024.0);

    // Verify system remained functional under memory pressure
    assert!(codex_ids.len() > NUM_CODICES / 2,
            "Should create at least half the codices despite chaos");

    // Test cleanup under pressure
    println!("Testing cleanup under memory pressure...");

    let cleanup_start = Instant::now();
    for codex_id in &codex_ids[0..10] {
        let deleted = manager.delete_codex(codex_id).await.unwrap();
        assert!(deleted, "Should be able to delete codices under pressure");
    }

    let cleanup_duration = cleanup_start.elapsed();
    println!("  Cleanup duration: {:?}", cleanup_duration);

    // Verify cleanup was effective
    let final_codex_count = manager.list_codices().await.len();
    assert_eq!(final_codex_count, codex_ids.len() - 10,
               "Cleanup should reduce codex count");

    let harness_stats = harness.get_stats();
    println!("Chaos statistics: {:?}", harness_stats);
}

#[tokio::test]
async fn test_network_partition_simulation() {
    // Simulate network partitions by introducing delays and failures
    let chaos_config = ChaosConfig {
        failure_rate: 0.3, // High failure rate to simulate partitions
        max_delay_ms: 1000, // High delays to simulate network issues
        network_partitions: true,
        ..Default::default()
    };

    let service = Arc::new(FailingService::new(chaos_config));

    // Simulate multiple nodes trying to communicate
    const NUM_NODES: usize = 5;
    const OPERATIONS_PER_NODE: usize = 20;

    println!("Starting network partition simulation with {} nodes", NUM_NODES);

    let start_time = Instant::now();
    let mut handles = Vec::new();

    for node_id in 0..NUM_NODES {
        let service_clone = service.clone();

        let handle = tokio::spawn(async move {
            let mut successful_communications = 0;
            let mut failed_communications = 0;
            let mut partition_events = 0;

            for op_id in 0..OPERATIONS_PER_NODE {
                let operation = format!("node_{}_communicate_{}", node_id, op_id);

                // Simulate communication attempt
                match service_clone.call(&operation).await {
                    Ok(_) => successful_communications += 1,
                    Err(e) => {
                        failed_communications += 1;
                        if e.to_string().contains("Simulated failure") {
                            partition_events += 1;
                        }
                    }
                }

                // Simulate work between communications
                sleep(Duration::from_millis(50)).await;
            }

            (successful_communications, failed_communications, partition_events)
        });

        handles.push(handle);
    }

    // Collect results
    let mut total_successful = 0;
    let mut total_failed = 0;
    let mut total_partition_events = 0;

    for handle in handles {
        let (successful, failed, partitions) = handle.await.unwrap();
        total_successful += successful;
        total_failed += failed;
        total_partition_events += partitions;
    }

    let duration = start_time.elapsed();

    println!("Network partition simulation results:");
    println!("  Total communication attempts: {}", NUM_NODES * OPERATIONS_PER_NODE);
    println!("  Successful communications: {}", total_successful);
    println!("  Failed communications: {}", total_failed);
    println!("  Partition events: {}", total_partition_events);
    println!("  Duration: {:?}", duration);
    println!("  Success rate: {:.2}%",
             (total_successful as f64 / (NUM_NODES * OPERATIONS_PER_NODE) as f64) * 100.0);

    // Verify network partition simulation worked
    assert!(total_partition_events > 0, "Should have simulated partition events");
    assert!(total_successful > 0, "Some communications should succeed despite partitions");

    // Test partition recovery
    println!("Testing partition recovery...");

    // Lower failure rate to simulate partition healing
    let recovery_service = Arc::new(FailingService::new(ChaosConfig {
        failure_rate: 0.1, // Much lower failure rate
        max_delay_ms: 50,
        ..Default::default()
    }));

    let mut recovery_successes = 0;
    for i in 0..10 {
        match recovery_service.call(&format!("recovery_test_{}", i)).await {
            Ok(_) => recovery_successes += 1,
            Err(_) => {}
        }
        sleep(Duration::from_millis(100)).await;
    }

    println!("Recovery test: {} successes out of 10 attempts", recovery_successes);
    assert!(recovery_successes > 7, "Should have high success rate after partition recovery");
}

#[tokio::test]
async fn test_comprehensive_chaos_scenarios() {
    println!("Running comprehensive chaos engineering test suite...");

    // Test multiple chaos scenarios simultaneously
    let chaos_config = ChaosConfig {
        failure_rate: 0.2,
        max_delay_ms: 100,
        memory_pressure: true,
        network_partitions: true,
        resource_exhaustion: true,
        seed: Some(123), // Fixed seed for reproducibility
    };

    let config = BinderyConfig::builder()
        .memory_limits(500, 120) // Moderate limits
        .unwrap()
        .collaboration(false, None::<String>, None::<String>)
        .unwrap()
        .build()
        .unwrap();

    let manager = Arc::new(CodexManager::with_config(config).unwrap());
    let service = Arc::new(FailingService::new(chaos_config.clone()));

    const DURATION_SECS: u64 = 10;
    const NUM_WORKERS: usize = 6;

    println!("Starting comprehensive chaos test for {} seconds with {} workers",
             DURATION_SECS, NUM_WORKERS);

    let start_time = Instant::now();
    let mut handles = Vec::new();

    // Spawn different types of workers
    for worker_id in 0..NUM_WORKERS {
        let manager_clone = manager.clone();
        let service_clone = service.clone();
        let mut worker_harness = ChaosTestHarness::new(chaos_config.clone());

        let handle = tokio::spawn(async move {
            let mut operations_completed = 0;
            let mut errors_encountered = 0;

            while start_time.elapsed().as_secs() < DURATION_SECS {
                // Randomize operation type
                let operation_type = worker_harness.rng.gen_range(0..4);

                match operation_type {
                    0 => {
                        // Codex operations
                        if !worker_harness.should_fail("codex_operation") {
                            match manager_clone.create_codex(
                                format!("Chaos Codex W{} T{}", worker_id, operations_completed),
                                "test_template"
                            ).await {
                                Ok(_) => operations_completed += 1,
                                Err(_) => errors_encountered += 1,
                            }
                        }
                    },
                    1 => {
                        // Service calls
                        match service_clone.call(&format!("chaos_call_{}_{}", worker_id, operations_completed)).await {
                            Ok(_) => operations_completed += 1,
                            Err(_) => errors_encountered += 1,
                        }
                    },
                    2 => {
                        // List operations
                        if !worker_harness.should_fail("list_operation") {
                            match manager_clone.list_codices().await {
                                _ => operations_completed += 1,
                            }
                        }
                    },
                    3 => {
                        // Memory operations
                        if !worker_harness.should_fail("memory_operation") {
                            let _stats = manager_clone.memory_stats().await;
                            operations_completed += 1;
                        }
                    },
                    _ => unreachable!(),
                }

                // Random delays
                worker_harness.simulate_delay().await;
            }

            (operations_completed, errors_encountered)
        });

        handles.push(handle);
    }

    // Background GC process
    let manager_gc = manager.clone();
    let gc_handle = tokio::spawn(async move {
        let mut gc_rounds = 0;
        while start_time.elapsed().as_secs() < DURATION_SECS {
            sleep(Duration::from_secs(2)).await;
            let _gc_stats = manager_gc.gc_all_codices().await.unwrap();
            gc_rounds += 1;
        }
        gc_rounds
    });

    // Collect results
    let mut total_operations = 0;
    let mut total_errors = 0;

    for handle in handles {
        let (operations, errors) = handle.await.unwrap();
        total_operations += operations;
        total_errors += errors;
    }

    let gc_rounds = gc_handle.await.unwrap();
    let duration = start_time.elapsed();

    println!("Comprehensive chaos test results:");
    println!("  Duration: {:?}", duration);
    println!("  Total operations: {}", total_operations);
    println!("  Total errors: {}", total_errors);
    println!("  GC rounds: {}", gc_rounds);
    println!("  Operations per second: {:.2}", total_operations as f64 / duration.as_secs_f64());
    println!("  Error rate: {:.2}%", (total_errors as f64 / (total_operations + total_errors) as f64) * 100.0);

    // Get final statistics
    let final_codex_count = manager.list_codices().await.len();
    let circuit_breaker_state = service.get_circuit_breaker_state().await;
    let service_stats = service.get_stats().await;

    println!("Final system state:");
    println!("  Codices created: {}", final_codex_count);
    println!("  Circuit breaker state: {}", circuit_breaker_state);
    println!("  Service statistics: {:?}", service_stats);

    // Verify system resilience
    assert!(total_operations > 0, "System should complete some operations despite chaos");
    assert!(final_codex_count > 0, "Should have created some codices");
    assert!(gc_rounds > 0, "Garbage collection should have run");

    // Test system recovery after chaos
    println!("Testing system recovery after chaos...");

    let recovery_start = Instant::now();
    let recovery_codex_id = manager.create_codex("Recovery Test Codex", "test_template").await.unwrap();
    let recovery_duration = recovery_start.elapsed();

    println!("Recovery test: created codex {} in {:?}", recovery_codex_id, recovery_duration);
    assert!(recovery_duration < Duration::from_secs(1), "System should recover quickly after chaos");
}